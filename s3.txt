What Is Amazon S3?
  * Amazon Simple Storage Service is storage for the Internet. 
  * It is designed to make web-scale computing easier for developers.
  * Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, 
    at any time, from anywhere on the web.
  * It gives any developer access to the same highly scalable, reliable, fast, 
    inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites. 
  * The service aims to maximize benefits of scale and to pass those benefits on to developers.
  
Advantages:

  - Create Buckets – Create and name a bucket that stores data. 
                     Buckets are the fundamental container in Amazon S3 for data storage.
  - Store data    – Store an infinite amount of data in a bucket. 
                    Upload as many objects as you like into an Amazon S3 bucket. 
                    Each object can contain up to 5 TB of data. 
                    Each object is stored and retrieved using a unique developer-assigned key.
  - Download data – Download your data or enable others to do so. 
                    Download your data any time you like or allow others to do the same.
  - Permissions – Grant or deny access to others who want to upload or download data into your Amazon S3 bucket. 
                  Grant upload and download permissions to three types of users. 
                  Authentication mechanisms can help keep data secure from unauthorized access.
  - Standard interfaces – Use standards-based REST and SOAP interfaces designed to work with any Internet-development toolkit.

Amazon S3 Concepts

  * Buckets - container for objects stored in Amazon S3. (max 100/aws account)
              Every object is contained in a bucket. 
              For example, if the object named photos/puppy.jpg is stored in the johnsmith bucket, 
              then it is addressable using the URL http://johnsmith.s3.amazonaws.com/photos/puppy.jpg
  * Objects - fundamental entities stored in Amazon S3. 
              Objects consist of object data and metadata. 
              The data portion is opaque(not seen) to Amazon S3. 
              The metadata is a set of name-value pairs that describe the object. 
              These include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. 
              You can also specify custom metadata at the time the object is stored.
              An object is uniquely identified within a bucket by a key (name) and a version ID
  * Keys   -  unique identifier for an object within a bucket. 
              Every object in a bucket has exactly one key. 
              Because the combination of a bucket, key, and version ID uniquely identify each object, 
               Amazon S3 can be thought of as a basic data map between "bucket + key + version" and the object itself. 
  * Regions - geographical region where Amazon S3 will store the buckets you create
  * Amazon S3 Data Consistency Model
     - Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all regions.
     - Updates to a single key are atomic. 
     - For example, if you PUT to an existing key, a subsequent read might return the old data(why? see below) or the updated data, 
       but it will never return corrupted or partial data.
     - Amazon S3 achieves high availability by replicating data across multiple servers within Amazon's data centers. 
     - If a PUT request is successful, your data is safely stored. 
     - However, information about the changes must replicate across Amazon S3, which can take some time, and so you might observe the following behaviors:
        - A process writes a new object to Amazon S3 and immediately lists keys within its bucket. 
          Until the change is fully propagated, the object might not appear in the list.
        - A process replaces an existing object and immediately attempts to read it. 
          Until the change is fully propagated, Amazon S3 might return the prior data.
        - A process deletes an existing object and immediately attempts to read it. 
          Until the deletion is fully propagated, Amazon S3 might return the deleted data.
        - A process deletes an existing object and immediately lists keys within its bucket. 
          Until the deletion is fully propagated, Amazon S3 might list the deleted object.
     - Amazon S3 does not currently support object locking. If two PUT requests are simultaneously made to the same key, the request with the latest time stamp wins. 
       If this is an issue, you will need to build an object-locking mechanism into your application.
  
Amazon S3 Features
  Storage Classes
  Bucket Policies
  AWS Identity and Access Management
  Access Control Lists
  Versioning
  Operations

  
Amazon S3 Default Encryption for S3 Buckets
  * Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. 
  * You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket. 
  * The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or AWS KMS-managed keys (SSE-KMS).
  * When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk in its data centers and decrypts it when you download the objects. 
  * Default encryption works with all existing and new S3 buckets. 
  * Without default encryption, to encrypt all objects stored in a bucket, you must include encryption information with every object storage request. 
  * You must also set up an S3 bucket policy to reject storage requests that don't include encryption information.
  * There are no new charges for using default encryption for S3 buckets. 
  * Requests to configure the default encryption feature incur standard Amazon S3 request charges.  
  
Amazon S3 Transfer Acceleration
  * Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances 
  * Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. 
  * As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.  
  
  Why Use Amazon S3 Transfer Acceleration?
  - You have customers that upload to a centralized bucket from all over the world
  - You transfer gigabytes to terabytes of data on a regular basis across continents.
  - You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.

Objects:
  Object Key and Metadata
  Storage Classes
  Subresources
  Versioning
  Object Tagging
  Lifecycle Management
  Cross-Origin Resource Sharing (CORS)
  Operations on Objects
      
  Storage Classes for Frequently Accessed Objects
    For performance-sensitive use cases and frequently accessed data, Amazon S3 provides the following storage classes:
      - STANDARD - The default storage class. 
                   If you don't specify the storage class when you upload an object, Amazon S3 assigns the STANDARD storage                       class.
      - REDUCED_REDUNDANCY - The Reduced Redundancy Storage (RRS) storage class is designed for noncritical, 
                  reproducible data that can be stored with less redundancy than the STANDARD storage class.

  Storage Classes for Infrequently Accessed Objects
    For example, you might choose the STANDARD_IA and ONEZONE_IA storage classes:
    - For storing backups.
    - For older data that is accessed infrequently  

    STANDARD_IA - Amazon S3 stores the object data redundantly across multiple geographically separated Availability Zones 
                  STANDARD_IA objects are resilient to the loss of an Availability Zone.
                  offers greater availability, durability, and resiliency than the ONEZONE_IA class.
    ONEZONE_IA  - Amazon S3 stores the object data in only one Availability Zone, which makes it less expensive
                  However, the data is not resilient to the physical loss of the Availability Zone resulting from disasters,                    such as earth quakes and floods. 
                  The ONEZONE_IA storage class is as durable as STANDARD_IA, but it is less available and less resilient.

  Storage Class That Automatically Optimizes Frequently and Infrequently Accessed Objects
    INTELLIGENT_TIERING 
      - storage class is designed to optimize storage costs by automatically moving data to the most cost-effective 
        storage access tier, without performance impact or operational overhead. 
      - delivers automatic cost savings by moving data on a granular object level between two access tiers,
        a frequent access tier and a lower-cost infrequent access tier, when access patterns change. 
      - ideal if you want to optimize storage costs automatically for long-lived data when access patterns 
        are unknown or unpredictable.


  GLACIER Storage Class
   GLACIER storage - suitable for archiving data where data access is infrequent. 
                     offers the same durability and resiliency as the STANDARD storage class.

  Object Lifecycle Management
   - To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their lifecycle. 
   - A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. 
   - There are two types of actions:
      - Transition actions - Define when objects transition to another storage class. 
                            eg, transition objects to the STANDARD_IA storage class 30 days after you created them, or                                  archive objects to the GLACIER storage class
                            There are costs associated with the lifecycle transition requests. 
      - Expiration actions - Define when objects expire. Amazon S3 deletes expired objects on your behalf.

Amazon S3 Inventory
  * Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage. 
  * You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and     regulatory needs. 
  * You can also simplify and speed up business workflows and big data jobs using Amazon S3 inventory, which provides a           scheduled alternative to the Amazon S3 synchronous List API operation.

Managing Access Permissions to Your Amazon S3 Resources
  * By default, all Amazon S3 resources—buckets, objects, and related subresources are private: 
  * only the resource owner, an AWS account that created it, can access the resource. 
  * The resource owner can optionally grant access permissions to others by writing an access policy.

  * Amazon S3 offers access policy options broadly categorized as resource-based policies and user policies. 
  * Access policies you attach to your resources (buckets and objects) are referred to as resource-based policies. 
  * For example, bucket policies and access control lists (ACLs) are resource-based policies.
  * You can also attach access policies to users in your account. These are called user policies.

Protecting Data in Amazon S3

  Protecting Data Using Encryption
    - Protecting Data Using Server-Side Encryption
    - Protecting Data Using Client-Side Encryption
  Using Versioning
  Introduction to Amazon S3 Object Lock
    - WORM (Write once Read Many) model
    - prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely
    - two ways to manage object retention: retention periods and legal holds. 
    - Object Lock works only in versioned buckets, and retention periods and legal holds apply to individual object versions
    - To use Amazon S3 Object Lock, you take the following steps:
      - Create a new bucket with S3 Object Lock enabled.
      - (Optional) Configure a default retention period for objects placed in the bucket.
      - Place the objects that you want to lock in the bucket.
      - Apply a retention period, a legal hold, or both, to the objects that you want to protect.

Batch Job:
  * Amazon S3 batch operations to perform large-scale batch operations on Amazon S3 objects.
  * Amazon S3 batch operations can execute a single operation on lists of Amazon S3 objects that you specify. 
  * A single job can perform the specified operation on billions of objects containing exabytes of data. 
  * Amazon S3 tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully     managed, auditable, serverless experience.
  

Hosting a Static Website on Amazon S3
  * You can host a static website on Amazon Simple Storage Service (Amazon S3). 
  * On a static website, individual webpages include static content. 
  * They might also contain client-side scripts. 
  * By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or           ASP.NET. Amazon S3 does not support server-side scripting.
  
  Website Endpoints
  * When you configure a bucket for website hosting, the website is available via the region-specific website endpoint.         * Website endpoints are different from the endpoints where you send REST API requests
  The two general forms of an Amazon S3 website endpoint are as follows:
   * bucket-name.s3-website-region.amazonaws.com
   * bucket-name.s3-website.region.amazonaws.com

   Required configurations:
     - Enabling Website Hosting
     - Configuring Index Document Support
     - Permissions Required for Website Access

  Optional configurations:
    - (Optional) Configuring Web Traffic Logging
    - (Optional) Custom Error Document Support
    - (Optional) Configuring a Webpage Redirect
   
